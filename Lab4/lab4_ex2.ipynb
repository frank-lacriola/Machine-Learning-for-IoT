{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab4_ex2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-Mdwi-l7tgV"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "class WindowGenerator:\n",
        "    def __init__(self, input_width, label_options, mean, std):\n",
        "        self.input_width = input_width  # the number of samples contained in a single window\n",
        "        self.label_options = label_options\n",
        "        self.mean = tf.reshape(tf.convert_to_tensor(mean), [1, 1, 2])  # sec arg is the shape and during training the first dim is batch\n",
        "        self.std = tf.reshape(tf.convert_to_tensor(std), [1, 1, 2])\n",
        "\n",
        "    def split_window(self, features):\n",
        "        # here the assumption is that features already contains 7 values\n",
        "        inputs = features[:, :-1, :]  # we leave the last one as label, remember we have the batch at the first dim\n",
        "\n",
        "        if self.label_options < 2:\n",
        "            labels = features[:, -1, self.label_options]\n",
        "            labels = tf.expand_dims(labels, -1)\n",
        "            num_labels = 1\n",
        "        else:\n",
        "            labels = features[:, -1, :]\n",
        "            num_labels = 2\n",
        "\n",
        "        inputs.set_shape([None, self.input_width, 2])  # we set the batch dim as None because it will be needed later\n",
        "        labels.set_shape([None, num_labels])  # same for labels\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "    def normalize(self, features):\n",
        "        features = (features - self.mean) / (self.std + 1.e-6)  # we add a small quantity to avoid divisions by zero\n",
        "\n",
        "        return features\n",
        "\n",
        "    def preprocess(self, features):\n",
        "        inputs, labels = self.split_window(features)\n",
        "        inputs = self.normalize(inputs)  # we don't need to normalize labels\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "    def make_dataset(self, data, train):\n",
        "        ds = tf.keras.preprocessing.timeseries_dataset_from_array( # this is what we need\n",
        "                data=data,\n",
        "                targets=None,\n",
        "                sequence_length=self.input_width+1,  # + label\n",
        "                sequence_stride=1,\n",
        "                batch_size=32)  # we'll try to change batch size later\n",
        "        ds = ds.map(self.preprocess)  # the preprocess will be applied to every batch\n",
        "        ds = ds.cache()\n",
        "\n",
        "        if train is True:\n",
        "            ds = ds.shuffle(100, reshuffle_each_iteration=True) # 100 is the shuffling buffer size\n",
        "\n",
        "        return ds\n",
        "\n",
        "\n",
        "def train_model(model, labels, train_ds, val_ds, test_ds, epochs=10, saved_model_dir=None, alpha=1):\n",
        "\n",
        "    final_units = max(1, labels)\n",
        "    print(alpha)\n",
        "    if model == 'mlp':\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Flatten(),\n",
        "            keras.layers.Dense(units=int(alpha*128)),\n",
        "            keras.layers.ReLU(),\n",
        "            keras.layers.Dense(units=int(alpha*128)),\n",
        "            keras.layers.ReLU(),\n",
        "            keras.layers.Dense(units=final_units)\n",
        "        ])\n",
        "    elif model == 'cnn':\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Conv1D(filters=int(alpha*64), kernel_size=3),\n",
        "            keras.layers.ReLU(),\n",
        "            keras.layers.Flatten(),\n",
        "            keras.layers.Dense(units=int(alpha*64)),\n",
        "            keras.layers.ReLU(),\n",
        "            keras.layers.Dense(units=final_units)\n",
        "        ])\n",
        "    elif model == 'lstm':\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.LSTM(units=int(alpha*64)),\n",
        "            keras.layers.Flatten(),\n",
        "            keras.layers.Dense(units=final_units)\n",
        "        ])\n",
        "\n",
        "    if labels >= 2:\n",
        "        new_mae = customMAE()\n",
        "        metrics = [new_mae]\n",
        "    else:\n",
        "        metrics = [keras.metrics.MeanAbsoluteError()]\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=keras.losses.MeanSquaredError(),\n",
        "        metrics=metrics\n",
        "    )\n",
        "\n",
        "    # let's train for 20 epochs\n",
        "\n",
        "    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
        "    test_loss, test_mae = model.evaluate(test_ds)\n",
        "    print(\"{} \\n The MAE is: {}\".format(model.summary(), test_mae))\n",
        "\n",
        "    if saved_model_dir is not None:\n",
        "        run_model = tf.function(lambda x: model(x))\n",
        "        concrete_func = run_model.get_concrete_function(tf.TensorSpec([1, 6, 2], tf.float32))\n",
        "        model.save(args.saved_model_dir, signatures=concrete_func)\n",
        "\n",
        "\n",
        "class customMAE(keras.metrics.Metric):\n",
        "    def __init__(self, name='custom_MAE', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        # MAE: we need the sum --> total, and the count --> count to compute the mean\n",
        "        self.total = self.add_weight('total', initializer='zero', shape=(2,))\n",
        "        self.count = self.add_weight('count', initializer='zeros')\n",
        "\n",
        "    # We have to iterate over all the dataset and update the state vars\n",
        "    # This is computed at every batch\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        error = tf.abs(y_pred-y_true)\n",
        "        error = tf.reduce_mean(error, axis=0)\n",
        "        self.total.assign_add(error)\n",
        "        self.count.assign_add(1.)\n",
        "        return\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.count.assign(tf.zeros_like(self.count))\n",
        "        self.total.assign(tf.zeros_like(self.total))\n",
        "        return\n",
        "\n",
        "    # after we have updated for all the dataset we return the result\n",
        "    def result(self):\n",
        "        result = tf.math.divide_no_nan(self.total, self.count)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--model', type=str, help='model name among mlp cnn and lstm')\n",
        "    parser.add_argument('--labels', type=int, default=0,\n",
        "                        help='0 for temp forecasting, 1 for hum forecasting, 2 or more for both')\n",
        "    parser.add_argument('--saved_model_dir', type=str, default=None)\n",
        "    parser.add_argument('--alpha', type=float, default=1, help='Width multiplaier for structured pruning via width scaling')\n",
        "    args = parser.parse_args(['--model=cnn', '--labels=2', '--saved_model_dir=cnn_struct_pruning_5', '--alpha=0.5'])\n",
        "\n",
        "    seed = 42\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    zip_path = tf.keras.utils.get_file(\n",
        "        origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "        fname='jena_climate_2009_2016.csv.zip',\n",
        "        extract=True,\n",
        "        cache_dir='.', cache_subdir='data')\n",
        "\n",
        "    csv_path, _ = os.path.splitext(zip_path)\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    column_indices = [2, 5]\n",
        "    columns = df.columns[column_indices]\n",
        "    data = df[columns].values.astype(np.float32)\n",
        "\n",
        "    n = len(data)\n",
        "    train_data = data[0:int(n * 0.7)]\n",
        "    val_data = data[int(n * 0.7):int(n * 0.9)]\n",
        "    test_data = data[int(n * 0.9):]\n",
        "\n",
        "    mean = train_data.mean(axis=0)\n",
        "    std = train_data.std(axis=0)\n",
        "\n",
        "    input_width = 6\n",
        "    LABEL_OPTIONS = args.labels\n",
        "\n",
        "    generator = WindowGenerator(input_width, LABEL_OPTIONS, mean, std)\n",
        "    train_ds = generator.make_dataset(train_data, True)\n",
        "    val_ds = generator.make_dataset(val_data, False)\n",
        "    test_ds = generator.make_dataset(test_data, False)\n",
        "\n",
        "    train_model(args.model, LABEL_OPTIONS, train_ds, val_ds, test_ds, 20,\n",
        "                args.saved_model_dir, args.alpha)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APFul3GIFERZ",
        "outputId": "7abda7fd-f2c1-440c-9f3c-ac2f35a074eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n",
            "Epoch 1/20\n",
            "9195/9200 [============================>.] - ETA: 0s - loss: 62.0796 - custom_MAE: 2.3945"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1841: UserWarning: Metric customMAE implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9200/9200 [==============================] - 83s 9ms/step - loss: 62.0468 - custom_MAE: 2.3936 - val_loss: 1.9633 - val_custom_MAE: 0.9774\n",
            "Epoch 2/20\n",
            "9200/9200 [==============================] - 21s 2ms/step - loss: 0.9514 - custom_MAE: 0.6004 - val_loss: 0.7542 - val_custom_MAE: 0.5502\n",
            "Epoch 3/20\n",
            "9200/9200 [==============================] - 25s 3ms/step - loss: 0.8403 - custom_MAE: 0.5461 - val_loss: 0.5847 - val_custom_MAE: 0.4302\n",
            "Epoch 4/20\n",
            "9200/9200 [==============================] - 24s 3ms/step - loss: 0.7210 - custom_MAE: 0.5066 - val_loss: 0.5877 - val_custom_MAE: 0.4320\n",
            "Epoch 5/20\n",
            "9200/9200 [==============================] - 25s 3ms/step - loss: 0.7004 - custom_MAE: 0.4941 - val_loss: 0.6705 - val_custom_MAE: 0.4748\n",
            "Epoch 6/20\n",
            "9200/9200 [==============================] - 24s 3ms/step - loss: 0.6879 - custom_MAE: 0.4803 - val_loss: 1.6774 - val_custom_MAE: 0.9351\n",
            "Epoch 7/20\n",
            "9200/9200 [==============================] - 25s 3ms/step - loss: 0.6767 - custom_MAE: 0.4734 - val_loss: 0.6134 - val_custom_MAE: 0.4343\n",
            "Epoch 8/20\n",
            "9200/9200 [==============================] - 22s 2ms/step - loss: 0.6818 - custom_MAE: 0.4733 - val_loss: 0.8561 - val_custom_MAE: 0.6019\n",
            "Epoch 9/20\n",
            "9200/9200 [==============================] - 25s 3ms/step - loss: 0.6475 - custom_MAE: 0.4591 - val_loss: 0.8103 - val_custom_MAE: 0.5344\n",
            "Epoch 10/20\n",
            "9200/9200 [==============================] - 26s 3ms/step - loss: 0.6332 - custom_MAE: 0.4528 - val_loss: 0.5857 - val_custom_MAE: 0.4195\n",
            "Epoch 11/20\n",
            "9200/9200 [==============================] - 26s 3ms/step - loss: 0.6404 - custom_MAE: 0.4573 - val_loss: 0.8339 - val_custom_MAE: 0.6500\n",
            "Epoch 12/20\n",
            "9200/9200 [==============================] - 29s 3ms/step - loss: 0.6222 - custom_MAE: 0.4494 - val_loss: 0.5851 - val_custom_MAE: 0.4220\n",
            "Epoch 13/20\n",
            "9200/9200 [==============================] - 29s 3ms/step - loss: 0.6143 - custom_MAE: 0.4410 - val_loss: 0.8787 - val_custom_MAE: 0.7060\n",
            "Epoch 14/20\n",
            "9200/9200 [==============================] - 29s 3ms/step - loss: 0.6169 - custom_MAE: 0.4480 - val_loss: 0.5282 - val_custom_MAE: 0.3835\n",
            "Epoch 15/20\n",
            "9200/9200 [==============================] - 28s 3ms/step - loss: 0.6144 - custom_MAE: 0.4408 - val_loss: 0.5329 - val_custom_MAE: 0.3895\n",
            "Epoch 16/20\n",
            "9200/9200 [==============================] - 30s 3ms/step - loss: 0.6075 - custom_MAE: 0.4422 - val_loss: 0.5471 - val_custom_MAE: 0.4178\n",
            "Epoch 17/20\n",
            "9200/9200 [==============================] - 30s 3ms/step - loss: 0.6062 - custom_MAE: 0.4422 - val_loss: 0.6381 - val_custom_MAE: 0.4617\n",
            "Epoch 18/20\n",
            "9200/9200 [==============================] - 29s 3ms/step - loss: 0.6062 - custom_MAE: 0.4417 - val_loss: 0.5660 - val_custom_MAE: 0.4303\n",
            "Epoch 19/20\n",
            "9200/9200 [==============================] - 28s 3ms/step - loss: 0.5946 - custom_MAE: 0.4349 - val_loss: 0.6931 - val_custom_MAE: 0.5253\n",
            "Epoch 20/20\n",
            "9200/9200 [==============================] - 31s 3ms/step - loss: 0.6060 - custom_MAE: 0.4434 - val_loss: 0.5112 - val_custom_MAE: 0.3644\n",
            "1315/1315 [==============================] - 8s 6ms/step - loss: 0.5755 - custom_MAE: 0.3991\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_1 (Conv1D)           (None, 4, 32)             224       \n",
            "                                                                 \n",
            " re_lu_2 (ReLU)              (None, 4, 32)             0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " re_lu_3 (ReLU)              (None, 32)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,418\n",
            "Trainable params: 4,418\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None \n",
            " The MAE is: [0.14625597 0.6520282 ]\n",
            "INFO:tensorflow:Assets written to: cnn_struct_pruning_5/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--model', type=str, help='model name among mlp cnn and lstm')\n",
        "    parser.add_argument('--labels', type=int, default=0,\n",
        "                        help='0 for temp forecasting, 1 for hum forecasting, 2 or more for both')\n",
        "    parser.add_argument('--saved_model_dir', type=str, default=None)\n",
        "    parser.add_argument('--alpha', type=float, default=1, help='Width multiplaier for structured pruning via width scaling')\n",
        "    args = parser.parse_args(['--model=cnn', '--labels=2', '--saved_model_dir=cnn_struct_pruning_75', '--alpha=0.75'])\n",
        "\n",
        "    seed = 42\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    zip_path = tf.keras.utils.get_file(\n",
        "        origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "        fname='jena_climate_2009_2016.csv.zip',\n",
        "        extract=True,\n",
        "        cache_dir='.', cache_subdir='data')\n",
        "\n",
        "    csv_path, _ = os.path.splitext(zip_path)\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    column_indices = [2, 5]\n",
        "    columns = df.columns[column_indices]\n",
        "    data = df[columns].values.astype(np.float32)\n",
        "\n",
        "    n = len(data)\n",
        "    train_data = data[0:int(n * 0.7)]\n",
        "    val_data = data[int(n * 0.7):int(n * 0.9)]\n",
        "    test_data = data[int(n * 0.9):]\n",
        "\n",
        "    mean = train_data.mean(axis=0)\n",
        "    std = train_data.std(axis=0)\n",
        "\n",
        "    input_width = 6\n",
        "    LABEL_OPTIONS = args.labels\n",
        "\n",
        "    generator = WindowGenerator(input_width, LABEL_OPTIONS, mean, std)\n",
        "    train_ds = generator.make_dataset(train_data, True)\n",
        "    val_ds = generator.make_dataset(val_data, False)\n",
        "    test_ds = generator.make_dataset(test_data, False)\n",
        "\n",
        "    train_model(args.model, LABEL_OPTIONS, train_ds, val_ds, test_ds, 20,\n",
        "                args.saved_model_dir, args.alpha)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrVbYId4Fuzu",
        "outputId": "1afe1300-413c-44a1-8093-4ea690c9b8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.75\n",
            "Epoch 1/20\n",
            "9188/9200 [============================>.] - ETA: 0s - loss: 47.5337 - custom_MAE: 2.0268"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1841: UserWarning: Metric customMAE implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9200/9200 [==============================] - 81s 9ms/step - loss: 47.4744 - custom_MAE: 2.0254 - val_loss: 1.8380 - val_custom_MAE: 1.0547\n",
            "Epoch 2/20\n",
            "9200/9200 [==============================] - 29s 3ms/step - loss: 0.9262 - custom_MAE: 0.5893 - val_loss: 0.6302 - val_custom_MAE: 0.4825\n",
            "Epoch 3/20\n",
            "9200/9200 [==============================] - 27s 3ms/step - loss: 0.8529 - custom_MAE: 0.5405 - val_loss: 0.6314 - val_custom_MAE: 0.4577\n",
            "Epoch 4/20\n",
            "9200/9200 [==============================] - 27s 3ms/step - loss: 0.7260 - custom_MAE: 0.4998 - val_loss: 0.5694 - val_custom_MAE: 0.4283\n",
            "Epoch 5/20\n",
            "9200/9200 [==============================] - 29s 3ms/step - loss: 0.7107 - custom_MAE: 0.4926 - val_loss: 0.6931 - val_custom_MAE: 0.5133\n",
            "Epoch 6/20\n",
            "9200/9200 [==============================] - 26s 3ms/step - loss: 0.6938 - custom_MAE: 0.4807 - val_loss: 2.1906 - val_custom_MAE: 1.2628\n",
            "Epoch 7/20\n",
            "9200/9200 [==============================] - 26s 3ms/step - loss: 0.7005 - custom_MAE: 0.4775 - val_loss: 0.7416 - val_custom_MAE: 0.4849\n",
            "Epoch 8/20\n",
            "9200/9200 [==============================] - 28s 3ms/step - loss: 0.7199 - custom_MAE: 0.4843 - val_loss: 0.9244 - val_custom_MAE: 0.6704\n",
            "Epoch 9/20\n",
            "9200/9200 [==============================] - 25s 3ms/step - loss: 0.6769 - custom_MAE: 0.4726 - val_loss: 0.8097 - val_custom_MAE: 0.5484\n",
            "Epoch 10/20\n",
            "9200/9200 [==============================] - 27s 3ms/step - loss: 0.6459 - custom_MAE: 0.4646 - val_loss: 0.5963 - val_custom_MAE: 0.4196\n",
            "Epoch 11/20\n",
            "9200/9200 [==============================] - 26s 3ms/step - loss: 0.6729 - custom_MAE: 0.4726 - val_loss: 0.6708 - val_custom_MAE: 0.5479\n",
            "Epoch 12/20\n",
            "9200/9200 [==============================] - 26s 3ms/step - loss: 0.6406 - custom_MAE: 0.4588 - val_loss: 0.5876 - val_custom_MAE: 0.4788\n",
            "Epoch 13/20\n",
            "9200/9200 [==============================] - 27s 3ms/step - loss: 0.6343 - custom_MAE: 0.4513 - val_loss: 0.5398 - val_custom_MAE: 0.3975\n",
            "Epoch 14/20\n",
            "9200/9200 [==============================] - 24s 3ms/step - loss: 0.6246 - custom_MAE: 0.4544 - val_loss: 0.5431 - val_custom_MAE: 0.4096\n",
            "Epoch 15/20\n",
            "9200/9200 [==============================] - 27s 3ms/step - loss: 0.6436 - custom_MAE: 0.4542 - val_loss: 0.5870 - val_custom_MAE: 0.4334\n",
            "Epoch 16/20\n",
            "9200/9200 [==============================] - 27s 3ms/step - loss: 0.6202 - custom_MAE: 0.4511 - val_loss: 0.5864 - val_custom_MAE: 0.4751\n",
            "Epoch 17/20\n",
            "9200/9200 [==============================] - 27s 3ms/step - loss: 0.6251 - custom_MAE: 0.4541 - val_loss: 0.6138 - val_custom_MAE: 0.4429\n",
            "Epoch 18/20\n",
            "9200/9200 [==============================] - 34s 4ms/step - loss: 0.6333 - custom_MAE: 0.4525 - val_loss: 0.5301 - val_custom_MAE: 0.3907\n",
            "Epoch 19/20\n",
            "9200/9200 [==============================] - 31s 3ms/step - loss: 0.6075 - custom_MAE: 0.4428 - val_loss: 0.6508 - val_custom_MAE: 0.4510\n",
            "Epoch 20/20\n",
            "9200/9200 [==============================] - 27s 3ms/step - loss: 0.6052 - custom_MAE: 0.4452 - val_loss: 0.5533 - val_custom_MAE: 0.4407\n",
            "1315/1315 [==============================] - 8s 6ms/step - loss: 0.6103 - custom_MAE: 0.4625\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_2 (Conv1D)           (None, 4, 48)             336       \n",
            "                                                                 \n",
            " re_lu_4 (ReLU)              (None, 4, 48)             0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 192)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 48)                9264      \n",
            "                                                                 \n",
            " re_lu_5 (ReLU)              (None, 48)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 98        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,698\n",
            "Trainable params: 9,698\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None \n",
            " The MAE is: [0.2210647  0.70389044]\n",
            "INFO:tensorflow:Assets written to: cnn_struct_pruning_75/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TfLite model generation\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"cnn_struct_pruning_75/\")\n",
        "tflite_model = converter.convert()\n",
        "with open(\"cnn_prun_075.tflite\", 'wb') as fp:\n",
        "  fp.write(tflite_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXNaMR-dGVUx",
        "outputId": "0a00c1f5-8f93-4362-f2d6-d77005c66444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "h2kVdL2dYMEL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}