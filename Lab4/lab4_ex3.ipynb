{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab4_ex3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngDZ0E2SOD3_",
        "outputId": "58ed69db-4e25-4bb4-e05b-baff6b3bbd05"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_model_optimization\n",
            "  Downloading tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 213 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (0.1.6)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.19.5)\n",
            "Installing collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UvQ0DIlqJRt5"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import zlib\n",
        "\n",
        "\n",
        "class WindowGenerator:\n",
        "    def __init__(self, input_width, label_options, mean, std):\n",
        "        self.input_width = input_width  # the number of samples contained in a single window\n",
        "        self.label_options = label_options\n",
        "        self.mean = tf.reshape(tf.convert_to_tensor(mean), [1, 1, 2])  # sec arg is the shape and during training the first dim is batch\n",
        "        self.std = tf.reshape(tf.convert_to_tensor(std), [1, 1, 2])\n",
        "\n",
        "    def split_window(self, features):\n",
        "        # here the assumption is that features already contains 7 values\n",
        "        inputs = features[:, :-1, :]  # we leave the last one as label, remember we have the batch at the first dim\n",
        "\n",
        "        if self.label_options < 2:\n",
        "            labels = features[:, -1, self.label_options]\n",
        "            labels = tf.expand_dims(labels, -1)\n",
        "            num_labels = 1\n",
        "        else:\n",
        "            labels = features[:, -1, :]\n",
        "            num_labels = 2\n",
        "\n",
        "        inputs.set_shape([None, self.input_width, 2])  # we set the batch dim as None because it will be needed later\n",
        "        labels.set_shape([None, num_labels])  # same for labels\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "    def normalize(self, features):\n",
        "        features = (features - self.mean) / (self.std + 1.e-6)  # we add a small quantity to avoid divisions by zero\n",
        "\n",
        "        return features\n",
        "\n",
        "    def preprocess(self, features):\n",
        "        inputs, labels = self.split_window(features)\n",
        "        inputs = self.normalize(inputs)  # we don't need to normalize labels\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "    def make_dataset(self, data, train):\n",
        "        ds = tf.keras.preprocessing.timeseries_dataset_from_array( # this is what we need\n",
        "                data=data,\n",
        "                targets=None,\n",
        "                sequence_length=self.input_width+1,  # + label\n",
        "                sequence_stride=1,\n",
        "                batch_size=32)  # we'll try to change batch size later\n",
        "        ds = ds.map(self.preprocess)  # the preprocess will be applied to every batch\n",
        "        ds = ds.cache()\n",
        "\n",
        "        if train is True:\n",
        "            ds = ds.shuffle(100, reshuffle_each_iteration=True) # 100 is the shuffling buffer size\n",
        "\n",
        "        return ds\n",
        "\n",
        "\n",
        "def train_model(model, labels, train_ds, val_ds, test_ds, epochs=10, saved_model_dir=None, alpha=1):\n",
        "\n",
        "    final_units = max(1, labels)\n",
        "    print(alpha)\n",
        "    if model == 'mlp':\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Flatten(),\n",
        "            keras.layers.Dense(units=int(alpha*128)),\n",
        "            keras.layers.ReLU(),\n",
        "            keras.layers.Dense(units=int(alpha*128)),\n",
        "            keras.layers.ReLU(),\n",
        "            keras.layers.Dense(units=final_units)\n",
        "        ])\n",
        "    elif model == 'cnn':\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Conv1D(filters=int(alpha*64), kernel_size=3),\n",
        "            keras.layers.ReLU(),\n",
        "            keras.layers.Flatten(),\n",
        "            keras.layers.Dense(units=int(alpha*64)),\n",
        "            keras.layers.ReLU(),\n",
        "            keras.layers.Dense(units=final_units)\n",
        "        ])\n",
        "    elif model == 'lstm':\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.LSTM(units=int(alpha*64)),\n",
        "            keras.layers.Flatten(),\n",
        "            keras.layers.Dense(units=final_units)\n",
        "        ])\n",
        "\n",
        "    if labels >= 2:\n",
        "        new_mae = customMAE()\n",
        "        metrics = [new_mae]\n",
        "    else:\n",
        "        metrics = [keras.metrics.MeanAbsoluteError()]\n",
        "\n",
        "  \n",
        "    pruning_params = {'pruning_schedule':\n",
        "      tfmot.sparsity.keras.PolynomialDecay(\n",
        "      initial_sparsity=0.30, \n",
        "      final_sparsity=0.8, \n",
        "      begin_step=len(train_ds)*5, # starts after 5 epochs  \n",
        "      end_step=len(train_ds)*15) # ends after 15 epochs\n",
        "    }\n",
        "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "    model = prune_low_magnitude(model, **pruning_params)  # new version which is able to perform pruning\n",
        "\n",
        "    # Pruning callback:\n",
        "    callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=keras.losses.MeanSquaredError(),\n",
        "        metrics=metrics\n",
        "    )\n",
        "\n",
        "    # let's train for 20 epochs\n",
        "    input_shape = [32, 6, 2]\n",
        "    model.build(input_shape)\n",
        "    history = model.fit(train_ds, epochs=20, validation_data=val_ds, callbacks=callbacks) \n",
        "\n",
        "\n",
        "    test_loss, test_mae = model.evaluate(test_ds)\n",
        "    print(\"{} \\n The MAE is: {}\".format(model.summary(), test_mae))\n",
        "\n",
        "    # after training we need to remove all the information related to pruning\n",
        "    model = tfmot.sparsity.keras.strip_pruning(model)\n",
        "\n",
        "\n",
        "    if saved_model_dir is not None:\n",
        "      converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "      tflite_model = converter.convert()\n",
        "      with open(saved_model_dir, 'wb') as fp:\n",
        "        tflite_compressed = zlib.compress(tflite_model)\n",
        "        fp.write(tflite_compressed)\n",
        "\n",
        "class customMAE(keras.metrics.Metric):\n",
        "    def __init__(self, name='custom_MAE', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        # MAE: we need the sum --> total, and the count --> count to compute the mean\n",
        "        self.total = self.add_weight('total', initializer='zero', shape=(2,))\n",
        "        self.count = self.add_weight('count', initializer='zeros')\n",
        "\n",
        "    # We have to iterate over all the dataset and update the state vars\n",
        "    # This is computed at every batch\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        error = tf.abs(y_pred-y_true)\n",
        "        error = tf.reduce_mean(error, axis=0)\n",
        "        self.total.assign_add(error)\n",
        "        self.count.assign_add(1.)\n",
        "        return\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.count.assign(tf.zeros_like(self.count))\n",
        "        self.total.assign(tf.zeros_like(self.total))\n",
        "        return\n",
        "\n",
        "    # after we have updated for all the dataset we return the result\n",
        "    def result(self):\n",
        "        result = tf.math.divide_no_nan(self.total, self.count)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--model', type=str, help='model name among mlp cnn and lstm')\n",
        "    parser.add_argument('--labels', type=int, default=0,\n",
        "                        help='0 for temp forecasting, 1 for hum forecasting, 2 or more for both')\n",
        "    parser.add_argument('--saved_model_dir', type=str, default=None)\n",
        "    parser.add_argument('--alpha', type=float, default=1, help='Width multiplaier for structured pruning via width scaling')\n",
        "    args = parser.parse_args(['--model=cnn', '--labels=2', '--saved_model_dir=cnn_magnit_prun_spars08.tflite'])\n",
        "\n",
        "    seed = 42\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    zip_path = tf.keras.utils.get_file(\n",
        "        origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "        fname='jena_climate_2009_2016.csv.zip',\n",
        "        extract=True,\n",
        "        cache_dir='.', cache_subdir='data')\n",
        "\n",
        "    csv_path, _ = os.path.splitext(zip_path)\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    column_indices = [2, 5]\n",
        "    columns = df.columns[column_indices]\n",
        "    data = df[columns].values.astype(np.float32)\n",
        "\n",
        "    n = len(data)\n",
        "    train_data = data[0:int(n * 0.7)]\n",
        "    val_data = data[int(n * 0.7):int(n * 0.9)]\n",
        "    test_data = data[int(n * 0.9):]\n",
        "\n",
        "    mean = train_data.mean(axis=0)\n",
        "    std = train_data.std(axis=0)\n",
        "\n",
        "    input_width = 6\n",
        "    LABEL_OPTIONS = args.labels\n",
        "\n",
        "    generator = WindowGenerator(input_width, LABEL_OPTIONS, mean, std)\n",
        "    train_ds = generator.make_dataset(train_data, True)\n",
        "    val_ds = generator.make_dataset(val_data, False)\n",
        "    test_ds = generator.make_dataset(test_data, False)\n",
        "\n",
        "    train_model(args.model, LABEL_OPTIONS, train_ds, val_ds, test_ds, 20, args.saved_model_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCJYB4pPJ36x",
        "outputId": "d734cfad-9bc8-4ecf-ed9c-8d679e908d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:218: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  aggregation=tf.VariableAggregation.MEAN)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:225: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  aggregation=tf.VariableAggregation.MEAN)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:238: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  trainable=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9184/9200 [============================>.] - ETA: 0s - loss: 40.8861 - custom_MAE: 1.8764"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1841: UserWarning: Metric customMAE implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9200/9200 [==============================] - 58s 6ms/step - loss: 40.8174 - custom_MAE: 1.8744 - val_loss: 0.7734 - val_custom_MAE: 0.5104\n",
            "Epoch 2/20\n",
            "2328/9200 [======>.......................] - ETA: 19s - loss: 1.0877 - custom_MAE: 0.6347"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y3k5YFp1OWHC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}